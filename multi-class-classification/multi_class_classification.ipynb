{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "multi_class_classification.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMqs90AKVfP3lZUT4x9Ipl6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khavitidala/vladilena-milize/blob/main/multi-class-classification/multi_class_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WN1ZQ-9vwTi"
      },
      "source": [
        "# Multiclass Classification Using BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "or8WGuCdwuOJ"
      },
      "source": [
        "## Install Transformers Library and Download Helper Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFOU4hp6yeo3"
      },
      "source": [
        "!pip install transformers\n",
        "!wget https://raw.githubusercontent.com/khavitidala/vladilena-milize/main/multi-class-classification/utils.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZApjv_Wew6PM"
      },
      "source": [
        "## Load Helper Code and Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3y6xNL36AIZ"
      },
      "source": [
        "from utils import *\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTfgA_q7w-lk"
      },
      "source": [
        "## Data Configuration and Hyperparameter Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kxa0hIkKyt15"
      },
      "source": [
        "i2w = {0:'C1', 1:'C2', 2:'C3', 3:'C4', 4:'C5'}\n",
        "w2i = {'C1':0, 'C2':1, 'C3':2, 'C4':3, 'C5':4}\n",
        "\n",
        "args = {}\n",
        "\n",
        "args['path_data'] = '/content/drive/MyDrive/PDT/dataset awal.xlsx'\n",
        "\n",
        "args['num_labels'] = 5\n",
        "args['valid_criterion'] = 'F1' # if F1 = avg. macro F1\n",
        "args['model_checkpoint'] = 'indobenchmark/indobert-base-p2'\n",
        "args[\"experiment_name\"] = \"indobert_phase2_default\" # Experiment name\n",
        "args[\"model_dir\"] =\"/content/drive/MyDrive/PDT/model/save/\" # Model directory\n",
        "args[\"max_seq_len\"] = 512 # Max number of tokens\n",
        "args[\"train_batch_size\"] = 8 # Batch size for training\n",
        "args[\"valid_batch_size\"] = 8 # Batch size for validation\n",
        "args[\"lr\"] = 4e-5 # Learning rate\n",
        "args[\"dataset\"] = 'dataset-awal'\n",
        "\n",
        "args['task'] = 'sequence_classification'\n",
        "args['forward_fn'] = forward_sequence_classification\n",
        "args['metrics_fn'] = document_multiclass_metrics_fn\n",
        "args['k_fold'] = 1\n",
        "args['word_tokenizer_class'] = TweetTokenizer\n",
        "args[\"max_norm\"] = 10.0 # Clipping gradient norm\n",
        "args[\"n_epochs\"] = 25 # 10 # Number of training epochs\n",
        "args[\"num_layers\"] = 12 # Number of layers\n",
        "args[\"device\"] = 'cuda' #\"Device (cuda or cpu)\")\n",
        "args[\"fp16\"] = \"\" # \"Set to O0, O1, O2 or O3 for fp16 training (see apex documentation)\")\n",
        "args[\"seed\"] = 42 # Seed\n",
        "args[\"step_size\"] = 1 # Step size\n",
        "args[\"early_stop\"] = 12 #3 # Step size\n",
        "args[\"gamma\"] = 0.5 # Gamma\n",
        "args[\"debug\"] = True # debugging mode\n",
        "args[\"force\"] = True # force to rewrite experiment folder\n",
        "args[\"no_special_token\"] = True # not adding special token as the input\n",
        "args[\"lower\"] = True # lower case"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZK-Weq4xFSk"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0G5yuPjhFPuu"
      },
      "source": [
        "def load_data(path_data):\n",
        "  df_train = pd.read_excel(path_data, sheet_name='data_train')\n",
        "  df_valid = pd.read_excel(path_data, sheet_name='data_test')\n",
        "  df_test = pd.read_excel(path_data, sheet_name='data_test')\n",
        "  df_train = df_train.dropna().reset_index(drop=True)\n",
        "  df_valid = df_valid.dropna().reset_index(drop=True)\n",
        "  df_test = df_test.dropna().reset_index(drop=True)\n",
        "  # Mengonversi label menjadi bilangan bulat, misal C1 menjadi 0, C2 menjadi 1, dst.\n",
        "  df_train.label = df_train.label.apply(lambda x:re.sub(' +', '', x))\n",
        "  df_valid.label = df_train.label.apply(lambda x:re.sub(' +', '', x))\n",
        "  df_test.label = df_test.label.apply(lambda x:re.sub(' +', '', x))\n",
        "  df_train.label = df_train.label.apply(lambda x:w2i[x])\n",
        "  df_valid.label = df_test.label.apply(lambda x:w2i[x])\n",
        "  df_test.label = df_test.label.apply(lambda x:w2i[x])\n",
        "  df_train.columns = ['text','label']\n",
        "  df_valid.columns = ['text','label']\n",
        "  df_test.columns = ['text','label']\n",
        "\n",
        "  return df_train, df_valid, df_test"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pl9DoS-7VBz"
      },
      "source": [
        "df_train, df_valid, df_test = load_data(args['path_data'])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_LoWMm_xHec"
      },
      "source": [
        "## Load and Fine Tune the Pretrained Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQIMgwGEyx5r"
      },
      "source": [
        "# Make sure cuda is deterministic\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "# create directory\n",
        "model_dir = '{}/{}/{}'.format(args[\"model_dir\"],args[\"dataset\"],args['experiment_name'])\n",
        "if not os.path.exists(model_dir):\n",
        "    os.makedirs(model_dir, exist_ok=True)\n",
        "elif args['force']:\n",
        "    print(f'overwriting model directory `{model_dir}`')\n",
        "else:\n",
        "    raise Exception(f'model directory `{model_dir}` already exists, use force if you want to overwrite the folder')\n",
        "\n",
        "# Set random seed\n",
        "set_seed(args['seed'])  # Added here for reproductibility    \n",
        "\n",
        "metrics_scores = []\n",
        "result_dfs = []\n",
        "\n",
        "# load model\n",
        "model, tokenizer, vocab_path, config_path = load_model(args)\n",
        "optimizer = optim.Adam(model.parameters(), lr=args['lr'])\n",
        "\n",
        "if args['fp16']:\n",
        "    from apex import amp  # Apex is only required if we use fp16 training\n",
        "    model, optimizer = amp.initialize(model, optimizer, opt_level=args['fp16'])\n",
        "\n",
        "if args['device'] == \"cuda\":\n",
        "    model = model.cuda()\n",
        "\n",
        "print(\"=========== TRAINING PHASE ===========\")\n",
        "\n",
        "train_dataset = DocumentMultiClassDataset(df_train, tokenizer, lowercase=args[\"lower\"], no_special_token=args['no_special_token'])\n",
        "train_loader = DocumentMultiClassDataLoader(dataset=train_dataset, max_seq_len=args['max_seq_len'], batch_size=args['train_batch_size'], num_workers=2, shuffle=False)  \n",
        "\n",
        "valid_dataset = DocumentMultiClassDataset(df_valid, tokenizer, lowercase=args[\"lower\"], no_special_token=args['no_special_token'])\n",
        "valid_loader = DocumentMultiClassDataLoader(dataset=valid_dataset, max_seq_len=args['max_seq_len'], batch_size=args['valid_batch_size'], num_workers=2, shuffle=False)\n",
        "\n",
        "test_dataset = DocumentMultiClassDataset(df_test, tokenizer, lowercase=args[\"lower\"], no_special_token=args['no_special_token'])\n",
        "test_loader = DocumentMultiClassDataLoader(dataset=test_dataset, max_seq_len=args['max_seq_len'], batch_size=args['valid_batch_size'], num_workers=2, shuffle=False)\n",
        "\n",
        "# Train\n",
        "train(args, model, train_loader=train_loader, valid_loader=valid_loader, optimizer=optimizer, forward_fn=args['forward_fn'], metrics_fn=args['metrics_fn'], valid_criterion=args['valid_criterion'], i2w=i2w, n_epochs=args['n_epochs'], evaluate_every=1, early_stop=args['early_stop'], step_size=args['step_size'], gamma=args['gamma'], model_dir=model_dir, exp_id=0)\n",
        "\n",
        "# Save Meta\n",
        "if vocab_path:\n",
        "    shutil.copyfile(vocab_path, f'{model_dir}/vocab.txt')\n",
        "if config_path:\n",
        "    shutil.copyfile(config_path, f'{model_dir}/config.json')\n",
        "\n",
        "# Load best model\n",
        "model.load_state_dict(torch.load(model_dir + \"/best_model_0.th\"))\n",
        "\n",
        "# Evaluate\n",
        "print(\"=========== EVALUATION PHASE ===========\")\n",
        "test_loss, test_metrics, test_hyp, test_label, test_seq = evaluate(args, model, data_loader=test_loader, forward_fn=args['forward_fn'], metrics_fn=args['metrics_fn'], i2w=i2w, is_test=True)\n",
        "\n",
        "metrics_scores.append(test_metrics)\n",
        "result_dfs.append(pd.DataFrame({\n",
        "    'seq':test_seq, \n",
        "    'hyp': test_hyp, \n",
        "    'label': test_label\n",
        "}))\n",
        "    \n",
        "result_df = pd.concat(result_dfs)\n",
        "metric_df = pd.DataFrame.from_records(metrics_scores)\n",
        "\n",
        "print('== Prediction Result ==')\n",
        "print(result_df.head())\n",
        "print()\n",
        "\n",
        "print('== Model Performance ==')\n",
        "print(metric_df.describe())\n",
        "\n",
        "result_df.to_excel(model_dir + \"/prediction_result.xlsx\")\n",
        "metric_df.describe().to_excel(model_dir + \"/evaluation_result.xlsx\")\n",
        "\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxNYq4CXxPEw"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eC9SDE3y0_y1"
      },
      "source": [
        "text = input(\"Masukkan contoh data uji : \")\n",
        "subwords = tokenizer.encode(text)\n",
        "subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\n",
        "\n",
        "logits = model(subwords)[0]\n",
        "label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()\n",
        "\n",
        "print(f'Text: {text} | Label : {i2w[label]} ({F.softmax(logits, dim=-1).squeeze()[label] * 100:.3f}%)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-YmwaS8xRhX"
      },
      "source": [
        "## Reference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HM-ANZfRvdyq"
      },
      "source": [
        "All codes here is just refactor from indoNLU:\n",
        "\n",
        "Bryan Wilie, Karissa Vincentio, Genta Indra Winata, Samuel Cahyawijaya, X. Li, Zhi Yuan Lim, S. Soleman, R. Mahendra, Pascale Fung, Syafri Bahar, & A. Purwarianti (2020). IndoNLU: Benchmark and Resources for Evaluating Indonesian Natural Language Understanding. In *Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing.*\n",
        "\n"
      ]
    }
  ]
}